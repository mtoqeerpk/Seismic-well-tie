{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integrate well logs divided in different files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this code you will be able to integrate DT and RHOB logs in one .las file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import las\n",
    "import numpy as np\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "las_files = ('perfis/1RJS_0051__RJ_1RJS_0051__RJ_BHC_00003.las', 'perfis/1RJS_0051__RJ_1RJS_0051__RJ_FDC_CNL_00001.las', 'perfis/1RJS_0051__RJ_1RJS_0051__RJ_FDC_CNL_GR_00004.las', 'perfis/1RJS_0051__RJ_1RJS_0051__RJ_IES_00005.las', 'perfis/1RJS_0051__RJ_1RJS_0051__RJ_ISF_BHC_GR_00002.las', 'perfis/1RJS_0051__RJ_1RJS_0051__RJ_SIGEO__013_00006.las')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_files = len(las_files)\n",
    "N_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(N_files):\n",
    "    globals()['log%s' % i] = las.LASReader(las_files[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1524"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log0.step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and assigning the logs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_logs(las_files):\n",
    "    \n",
    "    \n",
    "\n",
    "    N_files = len(las_files) \n",
    "\n",
    "    DT = []\n",
    "    RHOB = []\n",
    "    DEPT = []\n",
    "    DT_1 = []\n",
    "    DT_11 = []\n",
    "    DEPT_D = []\n",
    "    DEPT_R = []\n",
    "    RHOB_1 = []\n",
    "    RHOB_11 = []\n",
    "    data_DT = []\n",
    "    data_RHOB = []\n",
    "\n",
    "    for i in range(N_files):\n",
    "        if 'Sonic' in open(las_files[i]).read():\n",
    "            log = las.LASReader(las_files[i])\n",
    "\n",
    "            DT_11 = np.append(DT_11, log.data['DT'])\n",
    "            #DT_1 = np.append(DT_1, log.data['DT_1'])         # Add this command if your data have a collumns named DT_1\n",
    "            DEPT_D = np.append(DEPT_D, log.data['DEPT'])\n",
    "        \n",
    "    null = np.where(DT_11 == -999.25)              # finding the indices of the null values\n",
    "    #np.put(DT_11, [null], [DT_1[null]])         # not needed if you don't have DT_1 - it merges both DT logs\n",
    "    null_2 = np.where(DT_11 == -999.25)         # finding null values in both logs\n",
    "    DT_11[null_2] = 0.                        # replacing null values by 'nan'\n",
    "    \n",
    "    DT_2 = np.array([DT_11])\n",
    "    DEPT_2 = np.array([DEPT_D])\n",
    "    data_DT = np.concatenate((DEPT_2.T, DT_2.T), axis=1)\n",
    "    data_DT = data_DT[np.argsort(data_DT[:, 0])]\n",
    "    DT = data_DT[:,1]\n",
    "    \n",
    "    for i in range(N_files):\n",
    "        if 'Bulk density' in open(las_files[i]).read():\n",
    "            log = las.LASReader(las_files[i])\n",
    "            \n",
    "            RHOB_11 = np.append(RHOB_11, log.data['RHOB'])\n",
    "            #RHOB_1 = np.append(RHOB_1, log.data['RHOB_1'])      # Add this command if your data have a collumns named RHOB_1\n",
    "            DEPT_R = np.append(DEPT_R, log.data['DEPT'])\n",
    "            \n",
    "    null = np.where(RHOB_11 == -999.25)              # finding the indices of the null values\n",
    "    #np.put(RHOB_11, [null], [RHOB_1[null]])         # not needed if you don't have DT_1 - it merges both RHOB logs\n",
    "    null_2 = np.where(RHOB_11 == -999.25)         # finding null values in both logs\n",
    "    RHOB_11[null_2] = 0.                        # replacing null values by 'nan'\n",
    "\n",
    "    RHOB_2 = np.array([RHOB_11])\n",
    "    DEPT_2 = np.array([DEPT_R])\n",
    "    data_RHOB = np.concatenate((DEPT_2.T, RHOB_2.T), axis=1)\n",
    "    data_RHOB = data_RHOB[np.argsort(data_RHOB[:, 0])]\n",
    "    RHOB = data_RHOB[:,1]\n",
    "    \n",
    "    if DEPT_R.size > DEPT_D.size:\n",
    "        DEPT = data_RHOB[:,0]\n",
    "        D =  np.empty(DEPT.size,)\n",
    "        D.fill(np.nan)\n",
    "        idx_DT = np.asarray(np.where(DT >= 0))\n",
    "        D[(idx_DT[:])] = DT\n",
    "        DT = D\n",
    "\n",
    "    elif DEPT_D.size > DEPT_R.size:\n",
    "        DEPT = data_DT[:,0]\n",
    "        R =  np.empty(DEPT.size,)\n",
    "        R.fill(np.nan)\n",
    "        idx_RHOB = np.asarray(np.where(RHOB >= 0))\n",
    "        R[(idx_RHOB[:])] = RHOB\n",
    "        RHOB = R\n",
    "        \n",
    "    null_3 = np.where(DT == 0.)\n",
    "    DT[null_3] = np.nan \n",
    "    data_DT[null_3] = np.nan \n",
    "        \n",
    "    null_3 = np.where(RHOB == 0.)\n",
    "    RHOB[null_3] = np.nan  \n",
    "    data_RHOB[null_3] = np.nan \n",
    "    \n",
    "    return DEPT, DT, RHOB, data_DT, data_RHOB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DEPT, DT, RHOB, data_DT, data_RHOB = extract_logs(las_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.amin(data_DT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  5.7429e+02,   1.8759e+00],\n",
       "       [  5.7444e+02,   1.8448e+00],\n",
       "       [  5.7460e+02,   1.8203e+00],\n",
       "       ..., \n",
       "       [  3.5366e+03,   2.6824e+00],\n",
       "       [  3.5368e+03,   2.6799e+00],\n",
       "       [  3.5369e+03,   2.6827e+00]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_RHOB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19488"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEPT.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the start and end of the depth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DEPT_start = DEPT[0]\n",
    "DEPT_end = DEPT[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "574.292\n"
     ]
    }
   ],
   "source": [
    "print DEPT_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3532.8332\n"
     ]
    }
   ],
   "source": [
    "print DEPT_end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting integrated logs to a txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savetxt('DT_int.txt', data_DT, fmt='%s', delimiter='    ', newline='\\n', header='', footer='', comments='# ') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savetxt('RHOB_int.txt', data_RHOB, fmt='%s', delimiter='    ', newline='\\n', header='', footer='', comments='# ') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda2]",
   "language": "python",
   "name": "conda-env-anaconda2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
