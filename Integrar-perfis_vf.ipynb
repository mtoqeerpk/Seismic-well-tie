{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integrating well logs divided in different files and/or columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this code you will be able to integrate DT and RHOB logs in one .las file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import las\n",
    "import numpy as np\n",
    "import sys\n",
    "import lasio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy and paste here the address of your files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "las_files = ('perfis/1RJS_0051__RJ_1RJS_0051__RJ_BHC_00003.las', 'perfis/1RJS_0051__RJ_1RJS_0051__RJ_FDC_CNL_00001.las', 'perfis/1RJS_0051__RJ_1RJS_0051__RJ_FDC_CNL_GR_00004.las', 'perfis/1RJS_0051__RJ_1RJS_0051__RJ_IES_00005.las', 'perfis/1RJS_0051__RJ_1RJS_0051__RJ_ISF_BHC_GR_00002.las', 'perfis/1RJS_0051__RJ_1RJS_0051__RJ_SIGEO__013_00006.las')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_files = len(las_files)\n",
    "N_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(N_files):\n",
    "    globals()['log%s' % i] = las.LASReader(las_files[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1524"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log0.step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([( 2295.3452,   76.8985, -999.25  ),\n",
       "       ( 2295.4976,   76.2669, -999.25  ),\n",
       "       ( 2295.65  ,   77.0666, -999.25  ), ...,\n",
       "       ( 3536.6432, -999.25  ,    2.6824),\n",
       "       ( 3536.7956, -999.25  ,    2.6799),\n",
       "       ( 3536.948 , -999.25  ,    2.6827)],\n",
       "      dtype=[('DEPT', '<f8'), ('GR', '<f8'), ('RHOB', '<f8')])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log2.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and assigning the logs:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important!** This code needs to be customized for each well. Unfortunately I still didn't find a way to integrate all of them with a general code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "START = []\n",
    "END = []\n",
    "DEP = []\n",
    "DEPT = []\n",
    "\n",
    "N_files = len(las_files)\n",
    "\n",
    "for i in range(N_files):              # Creating an empty array indexed with the depth\n",
    "    \n",
    "    globals()['log_%s' % i] = las.LASReader(las_files[i])\n",
    "    np.set_printoptions(precision=4)\n",
    "    \n",
    "    globals()['DEPT_%s' % i] = globals()['log_%s' % i].data['DEPT']\n",
    "    \n",
    "    START_logs =  globals()['DEPT_%s' % i][0]\n",
    "    START = np.append(START, START_logs)\n",
    "    \n",
    "    END_logs =  globals()['DEPT_%s' % i][-1]\n",
    "    END = np.append(END, END_logs)\n",
    "    \n",
    "    DEPT_start = np.argmin(START)\n",
    "    DEPT_end = np.argmax(END)\n",
    "    \n",
    "    sample_int = globals()['log_%s' % i].well.STEP.value\n",
    "    N_samples = int((END[DEPT_end] - START[DEPT_start])/sample_int)+1\n",
    "    \n",
    "    Ini = START[DEPT_start] \n",
    "    \n",
    "DEPT = np.append(DEPT, Ini)\n",
    "    \n",
    "for i in range(N_samples):   \n",
    "    \n",
    "    DE = Ini + sample_int\n",
    "    DEP = np.append(DEP, DE)\n",
    "    Ini = DE     \n",
    "    \n",
    "DEPT = np.append(DEPT, DEP)\n",
    "\n",
    "\n",
    "\n",
    "counter = 0\n",
    "\n",
    "count_d = []\n",
    "count_r = []\n",
    "\n",
    "for i in range(N_files):                  # counting the number of columns the same log appears in each file\n",
    "        \n",
    "    with open(las_files[i]) as f:\n",
    "        contents = f.read()\n",
    "        count = contents.count(\"Sonic\")\n",
    "        count_d = np.append(count_d, count)\n",
    "        \n",
    "    with open(las_files[i]) as f:\n",
    "        contents = f.read()\n",
    "        count = contents.count(\"Bulk\")\n",
    "        count_r = np.append(count_r, count)\n",
    "          \n",
    "            \n",
    "            \n",
    "# Analysis of the DT log\n",
    "\n",
    "\n",
    "D = np.empty(N_samples+1,)          # Creating and empty array to receive the values\n",
    "D[:] = np.nan\n",
    "DT = D\n",
    "\n",
    "count_d_i = np.where(count_d>0)\n",
    "\n",
    "for i in range(np.asarray(count_d_i).size):\n",
    "        \n",
    "            \n",
    "    if count_d[count_d_i[0][i]] == 1:                          # verifies if the same log is divided in different files\n",
    "        \n",
    "        \n",
    "        globals()['log_S_%s' % i] = las.LASReader(las_files[count_d_i[0][i]])  # saves the logs which has DT log\n",
    "        \n",
    "        globals()['DEPT_S_%s' % i] = globals()['log_S_%s' % i].data['DEPT']   # Stores the depths of each file in different parameters\n",
    "               \n",
    "        \n",
    "        x = 'DT_'\n",
    "        y = i\n",
    "        z = count_d_i[0][i]\n",
    "            \n",
    "        globals()['%s%s%s' % (x,y,z)] = globals()['log_S_%s' % i].data['DT']\n",
    "        \n",
    "        xv = 'valid_S_'\n",
    "        yv = i\n",
    "        zv = count_d_i[0][i]\n",
    "        \n",
    "        globals()['%s%s%s' % (xv,yv,zv)] = np.where(globals()['%s%s%s' % (x,y,z)] > 0)   # finding the set of valid values \n",
    "\n",
    "        globals()['idx_v_S_%s' % i] = globals()['%s%s%s' % (xv,yv,zv)][0][0]\n",
    "\n",
    "        globals()['START_S_%s' % i] =  globals()['DEPT_S_%s' % i][globals()['idx_v_S_%s' % i]]  # finding the depth of the first valid value\n",
    "        \n",
    "        globals()['idx_S_start_%s' % i] = np.abs(globals()['START_S_%s' % i] - DEPT).argmin()   # index where the DT log starts in DEPT\n",
    "        \n",
    "        globals()['idx_v_E_%s' % i] = globals()['%s%s%s' % (xv,yv,zv)][0][-1] # finding the index of the last valid value \n",
    "        \n",
    "        globals()['END_S_%s' % i] =  globals()['DEPT_S_%s' % i][globals()['idx_v_E_%s' % i]]   # finding the depth of the last valid value\n",
    "        \n",
    "        globals()['idx_S_end_%s' % i]  = np.abs(globals()['END_S_%s' % i] - DEPT).argmin()   # index where the DT log ends in DEPT\n",
    "\n",
    "        \n",
    "        DT[(globals()['idx_S_start_%s' % i]):(globals()['idx_S_end_%s' % i])+1] = globals()['%s%s%s' % (x,y,z)][globals()['%s%s%s' % (xv,yv,zv)][0][0]:globals()['%s%s%s' % (xv,yv,zv)][0][-1]+1]\n",
    "        \n",
    "        \n",
    "    if count_d[count_d_i[0][i]] > 1:   # verifies if the same log is divided in different columns\n",
    "        \n",
    "        globals()['log_S_%s' % i] = las.LASReader(las_files[count_d_i[0][i]])  # saves the logs which has DT log\n",
    "        \n",
    "        globals()['DEPT_S_%s' % i] = globals()['log_S_%s' % i].data['DEPT']   # Stores the depths of each file in different parameters\n",
    "               \n",
    "        \n",
    "        x = 'DT_'\n",
    "        y = i\n",
    "        z = count_d_i[0][i]\n",
    "                \n",
    "        globals()['%s%s%s' % (x,y,z)] = globals()['log_S_%s' % i].data['DT']  # reading the first column\n",
    "                \n",
    "        xv = 'valid_S_'\n",
    "        yv = i\n",
    "        zv = count_d_i[0][i]\n",
    "        \n",
    "        globals()['%s%s%s' % (xv,yv,zv)] = np.where(globals()['%s%s%s' % (x,y,z)] > 0)\n",
    "          \n",
    "        globals()['idx_v_S_%s' % i] = globals()['%s%s%s' % (xv,yv,zv)][0][0]      # finding the index of the first valid value\n",
    "\n",
    "        globals()['START_S_%s' % i] =  globals()['DEPT_S_%s' % (i)][globals()['idx_v_S_%s' % i]]  # finding the depth of the first valid value\n",
    "        \n",
    "        globals()['idx_S_start_%s' % i] = np.abs(globals()['START_S_%s' % i] - DEPT).argmin()   # index where the RHOB log starts in DEPT\n",
    "\n",
    "        globals()['idx_v_E_%s' % i] = globals()['%s%s%s' % (xv,yv,zv)][0][-1] # finding the index of the last valid value\n",
    "          \n",
    "        globals()['END_S_%s' % i] =  globals()['DEPT_S_%s' % i][globals()['idx_v_E_%s' % i]]   # finding the depth of the last valid value\n",
    "        \n",
    "        globals()['idx_S_end_%s' % i]  = np.abs(globals()['END_S_%s' % i] - DEPT).argmin()   # index where the DT log ends in DEPT\n",
    "                  \n",
    "        DT[(globals()['idx_S_start_%s' % i]):(globals()['idx_S_end_%s' % i])+1] = globals()['%s%s%s' % (x,y,z)][globals()['%s%s%s' % (xv,yv,zv)][0][0]:globals()['%s%s%s' % (xv,yv,zv)][0][-1]+1]\n",
    "     \n",
    "    \n",
    "   \n",
    "        \n",
    "        for i in range(int(count_d[count_d_i[0][i]]) -1):  # Stores the logs divided in different columns in different parameters\n",
    "                \n",
    "            x = 'DT_'\n",
    "            y = i\n",
    "            z = count_d_i[0][i]\n",
    "        \n",
    "            globals()['DT_%s' % (i+1)] = globals()['log_S_%s' % i].data['DT_%s' % (i+1)]\n",
    "            \n",
    "                \n",
    "            globals()['valid_s_%s' % (i+1)] = np.where(globals()['DT_%s' % (i+1)] > 0)\n",
    "                    \n",
    "            globals()['%s%s%s' % (x,y,z)][globals()['valid_s_%s' % (i+1)][:]] = globals()['DT_%s' % (i+1)][globals()['valid_s_%s' % (i+1)]]    \n",
    "\n",
    "            globals()['idx_v_S_%s' % i] = globals()['valid_s_%s' % (i+1)][0][0]      # finding the index of the first valid value\n",
    "            \n",
    "            globals()['START_S_%s' % i] =  globals()['DEPT_S_%s' % (i)][globals()['idx_v_S_%s' % i]]  # finding the depth of the first valid value\n",
    "                      \n",
    "            globals()['idx_S_start_%s' % i] = np.abs(globals()['START_S_%s' % i] - DEPT).argmin()   # index where the RHOB log starts in DEPT\n",
    "            \n",
    "            globals()['idx_v_E_%s' % i] = globals()['valid_s_%s' % (i+1)][0][-1] # finding the index of the last valid value\n",
    "\n",
    "            globals()['END_S_%s' % i] =  globals()['DEPT_S_%s' % i][globals()['idx_v_E_%s' % i]]   # finding the depth of the last valid value\n",
    "            \n",
    "            globals()['idx_S_end_%s' % i]  = np.abs(globals()['END_S_%s' % i] - DEPT).argmin()   # index where the DT log ends in DEPT\n",
    "        \n",
    "                     \n",
    "            DT[(globals()['idx_S_start_%s' % i]):(globals()['idx_S_end_%s' % i])+1] = globals()['%s%s%s' % (x,y,z)][globals()['valid_s_%s' % (i+1)][0][0]:globals()['valid_s_%s' % (i+1)][0][-1]+1]\n",
    "               \n",
    "\n",
    "null = np.where(np.nan_to_num(DT)<0)\n",
    "DT[null] = np.nan\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Analysis of the RHOB log\n",
    "\n",
    "\n",
    "\n",
    "R = np.empty(N_samples+1,)           # Creating an empty array to receive the values\n",
    "R[:] = np.nan\n",
    "RHOB = R\n",
    "\n",
    "count_r_i = np.where(count_r>0) \n",
    "\n",
    "for i in range(np.asarray(count_r_i).size):\n",
    "        \n",
    "            \n",
    "    if count_r[count_r_i[0][i]] == 1:                          # verifies if the same log is divided in different files\n",
    "        \n",
    "        \n",
    "        globals()['log_B_%s' % i] = las.LASReader(las_files[count_r_i[0][i]])  # saves the logs which has DT log\n",
    "        \n",
    "        globals()['DEPT_B_%s' % i] = globals()['log_B_%s' % i].data['DEPT']   # Stores the depths of each file in different parameters\n",
    "               \n",
    "        \n",
    "        x = 'RHOB_'\n",
    "        y = i\n",
    "        z = count_r_i[0][i]\n",
    "            \n",
    "        globals()['%s%s%s' % (x,y,z)] = globals()['log_B_%s' % i].data['RHOB']\n",
    "        \n",
    "        xv = 'valid_R_'\n",
    "        yv = i\n",
    "        zv = count_r_i[0][i]\n",
    "        \n",
    "        globals()['%s%s%s' % (xv,yv,zv)] = np.where(globals()['%s%s%s' % (x,y,z)] > 0)   # finding the set of valid values \n",
    "\n",
    "        globals()['idx_v_B_%s' % i] = globals()['%s%s%s' % (xv,yv,zv)][0][0]\n",
    "\n",
    "        globals()['START_B_%s' % i] =  globals()['DEPT_B_%s' % i][globals()['idx_v_B_%s' % i]]  # finding the depth of the first valid value\n",
    "        \n",
    "        globals()['idx_B_start_%s' % i] = np.abs(globals()['START_B_%s' % i] - DEPT).argmin()   # index where the DT log starts in DEPT\n",
    "        \n",
    "        globals()['idx_v_E_%s' % i] = globals()['%s%s%s' % (xv,yv,zv)][0][-1] # finding the index of the last valid value \n",
    "        \n",
    "        globals()['END_B_%s' % i] =  globals()['DEPT_B_%s' % i][globals()['idx_v_E_%s' % i]]   # finding the depth of the last valid value\n",
    "        \n",
    "        globals()['idx_B_end_%s' % i]  = np.abs(globals()['END_B_%s' % i] - DEPT).argmin()   # index where the DT log ends in DEPT\n",
    "\n",
    "        \n",
    "        RHOB[(globals()['idx_B_start_%s' % i]):(globals()['idx_B_end_%s' % i])+1] = globals()['%s%s%s' % (x,y,z)][globals()['%s%s%s' % (xv,yv,zv)][0][0]:globals()['%s%s%s' % (xv,yv,zv)][0][-1]+1]\n",
    "        \n",
    "        \n",
    "    if count_r[count_r_i[0][i]] > 1:   # verifies if the same log is divided in different columns\n",
    "        \n",
    "        globals()['log_B_%s' % i] = las.LASReader(las_files[count_r_i[0][i]])  # saves the logs which has DT log\n",
    "        \n",
    "        globals()['DEPT_B_%s' % i] = globals()['log_B_%s' % i].data['DEPT']   # Stores the depths of each file in different parameters\n",
    "               \n",
    "        \n",
    "        x = 'RHOB_'\n",
    "        y = i\n",
    "        z = count_r_i[0][i]\n",
    "                \n",
    "        globals()['%s%s%s' % (x,y,z)] = globals()['log_B_%s' % i].data['RHOB']  # reading the first column\n",
    "                \n",
    "        xv = 'valid_B_'\n",
    "        yv = i\n",
    "        zv = count_r_i[0][i]\n",
    "        \n",
    "        globals()['%s%s%s' % (xv,yv,zv)] = np.where(globals()['%s%s%s' % (x,y,z)] > 0)\n",
    "          \n",
    "        globals()['idx_v_B_%s' % i] = globals()['%s%s%s' % (xv,yv,zv)][0][0]      # finding the index of the first valid value\n",
    "\n",
    "        globals()['START_B_%s' % i] =  globals()['DEPT_B_%s' % (i)][globals()['idx_v_B_%s' % i]]  # finding the depth of the first valid value\n",
    "        \n",
    "        globals()['idx_B_start_%s' % i] = np.abs(globals()['START_B_%s' % i] - DEPT).argmin()   # index where the RHOB log starts in DEPT\n",
    "\n",
    "        globals()['idx_v_E_%s' % i] = globals()['%s%s%s' % (xv,yv,zv)][0][-1] # finding the index of the last valid value\n",
    "          \n",
    "        globals()['END_B_%s' % i] =  globals()['DEPT_B_%s' % i][globals()['idx_v_E_%s' % i]]   # finding the depth of the last valid value\n",
    "        \n",
    "        globals()['idx_B_end_%s' % i]  = np.abs(globals()['END_B_%s' % i] - DEPT).argmin()   # index where the DT log ends in DEPT\n",
    "                  \n",
    "        \n",
    "        RHOB[(globals()['idx_B_start_%s' % i]):(globals()['idx_B_end_%s' % i])+1] = globals()['%s%s%s' % (x,y,z)][globals()['%s%s%s' % (xv,yv,zv)][0][0]:globals()['%s%s%s' % (xv,yv,zv)][0][-1]+1]\n",
    "     \n",
    "    \n",
    "   \n",
    "        \n",
    "        for i in range(int(count_r[count_r_i[0][i]]) -1):  # Stores the logs divided in different columns in different parameters\n",
    "                \n",
    "            x = 'RHOB_'\n",
    "            y = i\n",
    "            z = count_r_i[0][i]\n",
    "        \n",
    "            globals()['RHOB_%s' % (i+1)] = globals()['log_B_%s' % i].data['RHOB_%s' % (i+1)]\n",
    "            \n",
    "                \n",
    "            globals()['valid_b_%s' % (i+1)] = np.where(globals()['RHOB_%s' % (i+1)] > 0)\n",
    "                    \n",
    "            globals()['%s%s%s' % (x,y,z)][globals()['valid_b_%s' % (i+1)][:]] = globals()['RHOB_%s' % (i+1)][globals()['valid_b_%s' % (i+1)]]    \n",
    "\n",
    "            globals()['idx_v_B_%s' % i] = globals()['valid_b_%s' % (i+1)][0][0]      # finding the index of the first valid value\n",
    "            \n",
    "            globals()['START_B_%s' % i] =  globals()['DEPT_B_%s' % (i)][globals()['idx_v_B_%s' % i]]  # finding the depth of the first valid value\n",
    "                      \n",
    "            globals()['idx_B_start_%s' % i] = np.abs(globals()['START_B_%s' % i] - DEPT).argmin()   # index where the RHOB log starts in DEPT\n",
    "            \n",
    "            globals()['idx_v_E_%s' % i] = globals()['valid_b_%s' % (i+1)][0][-1] # finding the index of the last valid value\n",
    "\n",
    "            globals()['END_B_%s' % i] =  globals()['DEPT_B_%s' % i][globals()['idx_v_E_%s' % i]]   # finding the depth of the last valid value\n",
    "            \n",
    "            globals()['idx_B_end_%s' % i]  = np.abs(globals()['END_B_%s' % i] - DEPT).argmin()   # index where the DT log ends in DEPT\n",
    "        \n",
    "                     \n",
    "            RHOB[(globals()['idx_B_start_%s' % i]):(globals()['idx_B_end_%s' % i])+1] = globals()['%s%s%s' % (x,y,z)][globals()['valid_b_%s' % (i+1)][0][0]:globals()['valid_b_%s' % (i+1)][0][-1]+1]\n",
    "               \n",
    "\n",
    "\n",
    "\n",
    "null = np.where(np.nan_to_num(RHOB)<0)\n",
    "RHOB[null] = np.nan\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ nan,  nan,  nan, ...,  nan,  nan,  nan])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    nan,     nan,     nan, ...,  2.6799,  2.6827,     nan])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RHOB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  570.482 ,   570.6344,   570.7868, ...,  3536.7956,  3536.948 ,\n",
       "        3537.1004])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19467"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEPT.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the start and end of the depth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DEPT_start = DEPT[0]\n",
    "DEPT_end = DEPT[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "570.482\n"
     ]
    }
   ],
   "source": [
    "print DEPT_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3537.1004\n"
     ]
    }
   ],
   "source": [
    "print DEPT_end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting integrated logs to a txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  570.482 ],\n",
       "       [  570.6344],\n",
       "       [  570.7868],\n",
       "       ..., \n",
       "       [ 3536.7956],\n",
       "       [ 3536.948 ],\n",
       "       [ 3537.1004]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Depth = np.array([DEPT])\n",
    "Depth.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ nan],\n",
       "       [ nan],\n",
       "       [ nan],\n",
       "       ..., \n",
       "       [ nan],\n",
       "       [ nan],\n",
       "       [ nan]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DT = np.array([DT])\n",
    "DT.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    nan],\n",
       "       [    nan],\n",
       "       [    nan],\n",
       "       ..., \n",
       "       [ 2.6799],\n",
       "       [ 2.6827],\n",
       "       [    nan]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RHOB = np.array([RHOB])\n",
    "RHOB.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  5.7048e+02,          nan,          nan],\n",
       "       [  5.7063e+02,          nan,          nan],\n",
       "       [  5.7079e+02,          nan,          nan],\n",
       "       ..., \n",
       "       [  3.5368e+03,          nan,   2.6799e+00],\n",
       "       [  3.5369e+03,          nan,   2.6827e+00],\n",
       "       [  3.5371e+03,          nan,          nan]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logs_DT_RHOB_int = np.concatenate((Depth.T, DT.T, RHOB.T), axis=1)\n",
    "logs_DT_RHOB_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savetxt('logs_DT_RHOB_int.txt', (logs_DT_RHOB_int), fmt='%8.4f', delimiter='    ', newline='\\n', header='', footer='', comments='# ') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a .las file "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now, to import the data into an integration software you need to create a new .las file with the header and the integrated data. Pay attention to change at the header the depth related to the start and end of the well and replace the null value by 'nan'. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the header of well and create a new file .las, use the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_stdout = sys.stdout\n",
    "sys.stdout=open('out.las','w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "with open('perfis/1RJS_0051__RJ_1RJS_0051__RJ_BHC_00003.las', 'r') as f:\n",
    "    for line in f:\n",
    "        if line.lstrip().startswith('~A'):\n",
    "            break\n",
    "        f = np.append(f, (line.rstrip()))\n",
    "        print(line.rstrip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Insert the column RHOB to the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sys.stdout=open('out.las','a')\n",
    "print ('RHOB.                                                    :     3  Bulk density') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Insert the ASCII section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sys.stdout=open('out.las','a')\n",
    "print(line.rstrip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('logs_DT_RHOB_int.txt', 'r') as data:\n",
    "    for line in data:\n",
    "        if line.lstrip().startswith('~A'):\n",
    "            break\n",
    "        data = np.append(data, (line.rstrip()))\n",
    "        print(line.rstrip())\n",
    "data = (data, (line.rstrip()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing the value of some parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lasio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l = lasio.read('out.las')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "l.well['STRT'] = lasio.HeaderItem('STRT', value=DEPT_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l.well['STOP'] = lasio.HeaderItem('STOP', value=DEPT_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "l.well['NULL'] = lasio.HeaderItem('NULL', value='nan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "l.write('out_lasio.las', version=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda2]",
   "language": "python",
   "name": "conda-env-anaconda2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
